{"cells":[{"cell_type":"code","execution_count":1,"id":"bf1237d2","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer, StandardScaler, VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import lead\n","from pyspark.sql.window import Window"]},{"cell_type":"code","execution_count":2,"id":"b4a4f5d6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/12/01 01:41:59 INFO SparkEnv: Registering MapOutputTracker\n","24/12/01 01:41:59 INFO SparkEnv: Registering BlockManagerMaster\n","24/12/01 01:41:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/12/01 01:42:00 INFO SparkEnv: Registering OutputCommitCoordinator\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------------------+-------------+-----+-----+-----+------+---------+-----+-----+------------------+-------------------+------------------+\n","|           datetime|ticker_symbol| high|close| open|volume|    obv_0|mom_3|ema_3|bbands_3_upperband|bbands_3_middleband|bbands_3_lowerband|\n","+-------------------+-------------+-----+-----+-----+------+---------+-----+-----+------------------+-------------------+------------------+\n","|1998-01-30 00:00:00|       NVR.US|25.75|25.25|25.25| 21800| 145500.0| 1.69|24.76|             26.24|              24.52|              22.8|\n","|1998-06-09 00:00:00|       NVR.US|32.75|32.56|32.75| 37000|1210101.0| 0.43| 32.4|             32.86|              32.35|             31.85|\n","|1998-06-15 00:00:00|       NVR.US|33.69|33.25|33.25| 14400|1240801.0| 0.25|33.15|              33.4|              33.17|             32.93|\n","|1998-10-01 00:00:00|       NVR.US|32.88| 32.5|32.75|  7400|1775902.0|-3.25|33.29|             34.96|              33.31|             31.67|\n","|1998-10-23 00:00:00|       NVR.US|31.31|30.44|31.31| 32200|1715602.0| -1.0| 30.5|             31.18|              30.58|             29.98|\n","+-------------------+-------------+-----+-----+-----+------+---------+-----+-----+------------------+-------------------+------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"StockDataModel\").getOrCreate()\n","\n","# Define the path to the cleaned data in GCS\n","cleaned_data_path = \"gs://my-big-data-as/cleaned/*.parquet\"\n","\n","# Load the cleaned data into a Spark DataFrame (using a sample for testing)\n","df = spark.read.parquet(cleaned_data_path)\n","\n","# For testing, sample 2.5% of the data\n","df = df.sample(False, 0.025, seed=42)\n","\n","# Show the first few rows to understand the structure\n","df.show(5)"]},{"cell_type":"code","execution_count":3,"id":"f89fb226","metadata":{},"outputs":[],"source":["# Create a Window specification to calculate the next day's closing price\n","windowSpec = Window.partitionBy(\"ticker_symbol\").orderBy(\"datetime\")\n","\n","# Use the LEAD function to look ahead one day and get the closing price\n","df = df.withColumn(\"next_day_close\", lead(\"close\", 1).over(windowSpec))\n","\n","# Drop the last row for each group (since it will have NULL for the next day's close)\n","df = df.dropna(subset=[\"next_day_close\"])\n","\n","# Ensure columns are in double type for scaling\n","df = df.withColumn(\"volume\", df.volume.cast('double'))"]},{"cell_type":"code","execution_count":4,"id":"2cfb18ea","metadata":{},"outputs":[],"source":["# List of columns to scale\n","columns_to_scale = [\"high\", \"open\", \"volume\", \"obv_0\", \"mom_3\", \"ema_3\", \n","                    \"bbands_3_upperband\", \"bbands_3_middleband\", \"bbands_3_lowerband\"]\n","\n","# Assemble columns to scale into one vector in the pipeline (remove earlier manual transformation)\n","assembler_ = VectorAssembler(inputCols=columns_to_scale, outputCol=\"columns_to_scale_vector\")\n","\n","# Scale the vector using StandardScaler\n","scaler = StandardScaler(inputCol=\"columns_to_scale_vector\", outputCol=\"scaled_vector\", withStd=True, withMean=False)\n","\n","# StringIndexer for ticker_symbol (categorical feature)\n","indexer = StringIndexer(inputCol=\"ticker_symbol\", outputCol=\"ticker_index\")\n","\n","# Assemble the final features into a feature vector\n","final_assembler = VectorAssembler(\n","    inputCols=[\"scaled_vector\", \"ticker_index\"],  # Includes scaled features and ticker_index\n","    outputCol=\"features\"\n",")"]},{"cell_type":"code","execution_count":5,"id":"13d72c45","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Create the pipeline with all the stages\n","pipeline = Pipeline(stages=[assembler_, scaler, indexer, final_assembler])\n","\n","# Transform dataframe based on pipeline\n","df_transformed = pipeline.fit(df).transform(df)\n","\n","# Save transformed feature vectors to trusted folder before model\n","df_transformed.write.parquet(\"gs://my-big-data-as/trusted/transformed_feature_vectors\")"]},{"cell_type":"code","execution_count":7,"id":"6ed9ccba","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["24/12/01 02:36:38 WARN CacheManager: Asked to cache already cached data.\n","24/12/01 02:45:11 WARN DAGScheduler: Broadcasting large task binary with size 1906.6 KiB\n","24/12/01 02:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1299.9 KiB\n","24/12/01 02:54:17 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n","24/12/01 02:57:23 WARN DAGScheduler: Broadcasting large task binary with size 1786.4 KiB\n","24/12/01 03:03:51 WARN DAGScheduler: Broadcasting large task binary with size 1923.3 KiB\n","24/12/01 03:12:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n","24/12/01 03:22:13 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n","24/12/01 03:25:18 WARN DAGScheduler: Broadcasting large task binary with size 1336.3 KiB\n","24/12/01 03:31:42 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n","24/12/01 03:33:30 WARN DAGScheduler: Broadcasting large task binary with size 1669.0 KiB\n","24/12/01 03:41:22 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n","24/12/01 03:42:34 WARN DAGScheduler: Broadcasting large task binary with size 1767.2 KiB\n","24/12/01 03:50:44 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n","24/12/01 03:51:52 WARN DAGScheduler: Broadcasting large task binary with size 1733.9 KiB\n","24/12/01 04:00:13 WARN DAGScheduler: Broadcasting large task binary with size 2023.4 KiB\n","24/12/01 04:01:05 WARN DAGScheduler: Broadcasting large task binary with size 1409.2 KiB\n","24/12/01 04:09:37 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n","24/12/01 04:19:21 WARN DAGScheduler: Broadcasting large task binary with size 1870.4 KiB\n","24/12/01 04:24:41 WARN DAGScheduler: Broadcasting large task binary with size 1372.5 KiB\n","24/12/01 04:28:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n","24/12/01 04:32:00 WARN DAGScheduler: Broadcasting large task binary with size 1664.5 KiB\n","24/12/01 04:37:57 WARN DAGScheduler: Broadcasting large task binary with size 1601.4 KiB\n","24/12/01 04:44:37 WARN BlockManagerMaster: Failed to remove broadcast 378 with removeFromMaster = true - Block broadcast_378 does not exist\n","org.apache.spark.SparkException: Block broadcast_378 does not exist\n","\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2100) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2066) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.18.jar:?]\n","\tat scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.18.jar:?]\n","\tat scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) [scala-library-2.12.18.jar:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n","\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n","24/12/01 04:44:37 ERROR ContextCleaner: Error cleaning broadcast 378\n","org.apache.spark.SparkException: Exception thrown in awaitResult: \n","\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56) ~[spark-common-utils_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:222) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:395) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:49) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:82) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:256) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:204) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1358) [spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) [spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) [spark-core_2.12-3.5.1.jar:3.5.1]\n","Caused by: org.apache.spark.SparkException: Block broadcast_378 does not exist\n","\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2100) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2066) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.18.jar:?]\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:2038) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.18.jar:?]\n","\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.18.jar:?]\n","\tat scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.18.jar:?]\n","\tat scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.18.jar:?]\n","\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.18.jar:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n","\tat java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]\n"]},{"name":"stderr","output_type":"stream","text":["24/12/01 04:46:57 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n","24/12/01 04:56:14 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n","24/12/01 05:01:56 WARN DAGScheduler: Broadcasting large task binary with size 1449.6 KiB\n","24/12/01 05:05:34 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n","24/12/01 05:11:13 WARN DAGScheduler: Broadcasting large task binary with size 1669.6 KiB\n","24/12/01 05:15:14 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n","24/12/01 05:20:28 WARN DAGScheduler: Broadcasting large task binary with size 1661.6 KiB\n","24/12/01 05:23:47 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n","24/12/01 05:29:43 WARN DAGScheduler: Broadcasting large task binary with size 1857.8 KiB\n","24/12/01 05:32:46 WARN DAGScheduler: Broadcasting large task binary with size 1695.3 KiB\n","24/12/01 05:38:49 WARN DAGScheduler: Broadcasting large task binary with size 1554.6 KiB\n","24/12/01 05:41:53 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","24/12/01 05:51:03 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n","                                                                                \r"]}],"source":["# Define the Random Forest model\n","rf = RandomForestRegressor(featuresCol='features', labelCol='next_day_close', maxBins=2048)\n","\n","# Split the data into training and testing sets \n","train_data, test_data = df_transformed.randomSplit([0.8, 0.2], seed=49)\n","\n","# Cache training data to speed up cross-validation\n","train_data.cache()\n","\n","# Set up cross-validation with hyperparameter tuning\n","paramGrid = ParamGridBuilder() \\\n","    .addGrid(rf.numTrees, [10, 20]) \\\n","    .addGrid(rf.maxDepth, [5, 10]) \\\n","    .build()\n","\n","# Regression evaluator for RMSE\n","rmse_evaluator = RegressionEvaluator(labelCol=\"next_day_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n","\n","# Set up the CrossValidator with RandomForest model, parameter grid, and evaluator\n","cv = CrossValidator(estimator=rf,\n","                    estimatorParamMaps=paramGrid,\n","                    evaluator=rmse_evaluator,  # Evaluator for RMSE\n","                    numFolds=2)\n","\n","# Train the model using cross-validation\n","cvModel = cv.fit(train_data)\n","\n","# Make predictions on the test set\n","predictions = cvModel.transform(test_data)"]},{"cell_type":"code","execution_count":8,"id":"4ffe4c91","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Root Mean Squared Error (RMSE) = 21.115899648183415\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Mean Absolute Error (MAE) = 8.1807257765551\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["R-Squared (R2) = 0.9835247834224002\n","Feature Importances: \n","datetime: 0.3520112845975745\n","ticker_symbol: 0.06829989467833736\n","high: 0.0016100864946969075\n","close: 0.006082477935932407\n","open: 0.0008128456852295622\n","volume: 0.16883962746515507\n","obv_0: 0.18445926616383362\n","mom_3: 0.1318217733920169\n","ema_3: 0.08457997615903919\n","bbands_3_upperband: 0.0014827674281844242\n"]},{"name":"stderr","output_type":"stream","text":["24/12/01 06:09:12 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n","[Stage 410:========================================>           (779 + 4) / 1000]\r"]}],"source":["# Evaluate the model using RMSE, MAE, and R2\n","rmse = rmse_evaluator.evaluate(predictions)\n","print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n","\n","mae_evaluator = RegressionEvaluator(labelCol=\"next_day_close\", predictionCol=\"prediction\", metricName=\"mae\")\n","mae = mae_evaluator.evaluate(predictions)\n","print(f\"Mean Absolute Error (MAE) = {mae}\")\n","\n","r2_evaluator = RegressionEvaluator(labelCol=\"next_day_close\", predictionCol=\"prediction\", metricName=\"r2\")\n","r2 = r2_evaluator.evaluate(predictions)\n","print(f\"R-Squared (R2) = {r2}\")\n","\n","# Get feature importance from the best model\n","rf_model = cvModel.bestModel  # Best model after cross-validation\n","\n","# Extract feature importances\n","feature_importances = rf_model.featureImportances\n","\n","# Print the feature importances\n","print(\"Feature Importances: \")\n","for feature, importance in zip(df_transformed.columns, feature_importances):\n","    print(f\"{feature}: {importance}\")"]},{"cell_type":"code","execution_count":10,"id":"d7778f17","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/12/01 06:17:30 ERROR Instrumentation: org.apache.spark.SparkException: Job 207 cancelled because SparkContext was shut down\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n","\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n","\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n","\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n","\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1377)\n","\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n","\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2322)\n","\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1377)\n","\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2322)\n","\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2275)\n","\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2457)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2478)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2497)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2522)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n","\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n","\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n","\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n","\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n","\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n","\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n","\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/12/01 06:17:30 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.\n","org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.\n","\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690) ~[spark-core_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274) [spark-network-common_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111) [spark-network-common_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140) [spark-network-common_2.12-3.5.1.jar:3.5.1]\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) [spark-network-common_2.12-3.5.1.jar:3.5.1]\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) [netty-handler-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [netty-codec-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) [spark-network-common_2.12-3.5.1.jar:3.5.1]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [netty-transport-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.100.Final.jar:4.1.100.Final]\n","\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n","24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 413:============================================>       (857 + 8) / 1000]\r","24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n"]},{"name":"stderr","output_type":"stream","text":["24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/12/01 06:17:31 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n","This stopped SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","\n","The currently active SparkContext was created at:\n","\n","org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n","java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n","java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n","java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n","py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n","py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","py4j.Gateway.invoke(Gateway.java:238)\n","py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n","py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n","py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","java.base/java.lang.Thread.run(Thread.java:829)\n","         \n","\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n","\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2766)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n","\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n","\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:170)\n","\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n","\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n","\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:169)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:612)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:564)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:578)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n","\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n","\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n","\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:578)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:278)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:273)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n","\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:403)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n","\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3849)\n","\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3847)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:143)\n","\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n","\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n","\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n","\tat jdk.internal.reflect.GeneratedMethodAccessor202.invoke(Unknown Source)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n"]}],"source":["# Save the trained model to a location (optional)\n","rf_model.save(\"gs://my-big-data-as/models/stock_model\")\n","\n","# Optionally, save the test predictions to GCS\n","predictions.select(\"ticker_symbol\", \"datetime\", \"next_day_close\", \"prediction\").write.parquet(\"gs://my-big-data-as/models/test_predictions.parquet\")\n","\n","# Stop the Spark session\n","spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"4a4ce5b2","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}